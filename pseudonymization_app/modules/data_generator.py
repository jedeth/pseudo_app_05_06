#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
G√©n√©rateur de donn√©es d'entra√Ænement pour SpaCy
==============================================

Ce module g√©n√®re automatiquement des donn√©es d'entra√Ænement au format SpaCy
√† partir de listes de termes fournis par l'utilisateur. Il cr√©e des phrases
contextuelles r√©alistes pour chaque type d'entit√© NER personnalis√©e.
"""

import random
import json
import re
from pathlib import Path
from typing import List, Dict, Tuple, Any
from datetime import datetime

class TrainingDataGenerator:
    """
    G√©n√©rateur automatique de donn√©es d'entra√Ænement pour SpaCy NER
    
    Cette classe prend en entr√©e des listes de termes pour chaque entit√©
    personnalis√©e et g√©n√®re des phrases d'entra√Ænement contextuelles
    avec les annotations au format SpaCy.
    """
    
    def __init__(self):
        """
        Initialise le g√©n√©rateur avec des mod√®les de phrases pr√©d√©finis
        pour diff√©rents types d'entit√©s courantes
        """
        # Mod√®les de phrases par type d'entit√©
        # Chaque template contient {entity} qui sera remplac√© par le terme r√©el
        self.sentence_templates = {
            # Templates pour les √©tablissements (codes, identifiants)
            'ETABLISSEMENT': [
                "L'√©tablissement {entity} a √©t√© cr√©√© en 2020.",
                "Le code d'√©tablissement {entity} correspond √† notre filiale principale.",
                "Nous travaillons avec l'√©tablissement r√©f√©renc√© {entity}.",
                "L'identifiant {entity} d√©signe un √©tablissement certifi√©.",
                "Le site {entity} emploie plus de 50 personnes.",
                "L'√©tablissement {entity} respecte toutes les normes de qualit√©.",
                "Notre partenaire {entity} livre r√©guli√®rement nos produits.",
                "L'audit de l'√©tablissement {entity} aura lieu le mois prochain.",
                "Le code {entity} figure dans notre base de donn√©es clients.",
                "L'√©tablissement {entity} b√©n√©ficie d'une accr√©ditation sp√©ciale."
            ],
            
            # Templates pour les personnes (noms, pr√©noms, identifiants)
            'PERSONNE': [
                "Monsieur {entity} travaille dans ce service depuis 5 ans.",
                "Le dossier de {entity} n√©cessite une r√©vision urgente.",
                "Madame {entity} sera responsable de ce projet.",
                "{entity} a particip√© √† la r√©union d'hier.",
                "Le rapport de {entity} sera pr√©sent√© demain.",
                "Nous devons contacter {entity} rapidement.",
                "{entity} est expert dans son domaine.",
                "L'employ√© {entity} prend ses vacances en ao√ªt.",
                "Le Dr {entity} consulte tous les mardis.",
                "{entity} a sign√© le contrat ce matin."
            ],
            
            # Templates pour les organisations
            'ORGANISATION': [
                "L'organisation {entity} publie un rapport annuel.",
                "Nous collaborons avec {entity} depuis plusieurs ann√©es.",
                "{entity} organise une conf√©rence internationale.",
                "Le partenariat avec {entity} est tr√®s fructueux.",
                "{entity} a lanc√© une nouvelle initiative.",
                "Les membres de {entity} se r√©unissent chaque mois.",
                "{entity} contribue activement au d√©veloppement local.",
                "Le pr√©sident de {entity} donnera une interview.",
                "{entity} recrute de nouveaux collaborateurs.",
                "Les statuts de {entity} ont √©t√© modifi√©s r√©cemment."
            ],
            
            # Templates pour les lieux
            'LIEU': [
                "La r√©union aura lieu √† {entity}.",
                "Nous nous rendons souvent √† {entity}.",
                "{entity} est un endroit magnifique.",
                "Le si√®ge social se trouve √† {entity}.",
                "Les formations ont lieu √† {entity}.",
                "{entity} accueille de nombreux visiteurs.",
                "L'√©v√©nement se d√©roulera √† {entity}.",
                "Nous livrons r√©guli√®rement √† {entity}.",
                "{entity} dispose d'excellentes infrastructures.",
                "Le bureau de {entity} ferme √† 18h."
            ],
            
            # Templates pour les codes/identifiants g√©n√©riques
            'CODE': [
                "Le code {entity} est utilis√© pour identifier ce produit.",
                "Veuillez saisir le code {entity} dans le syst√®me.",
                "Le r√©f√©rence {entity} correspond √† cet article.",
                "L'identifiant {entity} permet de tracer l'op√©ration.",
                "Le num√©ro {entity} figure sur la facture.",
                "Utilisez le code {entity} pour acc√©der au service.",
                "Le syst√®me affiche l'erreur {entity}.",
                "La commande {entity} a √©t√© exp√©di√©e hier.",
                "Le ticket {entity} est en cours de traitement.",
                "La r√©f√©rence {entity} n'existe pas dans notre base."
            ]
        }
        
        # Mots de liaison et connecteurs pour enrichir les phrases
        self.connectors = [
            "En effet,", "Par ailleurs,", "De plus,", "Cependant,", "N√©anmoins,",
            "Ainsi,", "Par cons√©quent,", "En outre,", "D'autre part,", "Finalement,"
        ]
        
        # Compl√©ments contextuels
        self.context_additions = [
            "conform√©ment √† la r√©glementation",
            "selon nos proc√©dures internes",
            "dans le cadre de notre activit√©",
            "pour am√©liorer nos services",
            "afin de r√©pondre aux besoins",
            "en respect des normes en vigueur"
        ]
        
    def load_terms_from_file(self, filepath: str) -> List[str]:
        """
        Charge une liste de termes depuis un fichier texte
        
        Args:
            filepath (str): Chemin vers le fichier contenant les termes (un par ligne)
            
        Returns:
            List[str]: Liste des termes charg√©s et nettoy√©s
            
        Raises:
            FileNotFoundError: Si le fichier n'existe pas
            Exception: Pour toute autre erreur de lecture
        """
        try:
            terms = []
            with open(filepath, 'r', encoding='utf-8') as file:
                for line in file:
                    # Nettoie chaque ligne (supprime espaces et caract√®res sp√©ciaux)
                    term = line.strip()
                    if term and not term.startswith('#'):  # Ignore les lignes vides et commentaires
                        terms.append(term)
            
            print(f"‚úÖ {len(terms)} termes charg√©s depuis {filepath}")
            return terms
            
        except FileNotFoundError:
            raise FileNotFoundError(f"Le fichier {filepath} n'a pas √©t√© trouv√©")
        except Exception as e:
            raise Exception(f"Erreur lors de la lecture du fichier {filepath}: {str(e)}")
    
    def generate_sentence_for_term(self, term: str, entity_type: str, 
                                  add_context: bool = True) -> str:
        """
        G√©n√®re une phrase contextualis√©e pour un terme donn√©
        
        Args:
            term (str): Le terme √† int√©grer dans la phrase
            entity_type (str): Type d'entit√© (PERSONNE, ETABLISSEMENT, etc.)
            add_context (bool): Ajouter du contexte suppl√©mentaire
            
        Returns:
            str: Phrase g√©n√©r√©e contenant le terme
        """
        # S√©lectionne un template appropri√© ou utilise un template g√©n√©rique
        if entity_type in self.sentence_templates:
            templates = self.sentence_templates[entity_type]
        else:
            # Template g√©n√©rique si le type d'entit√© n'est pas reconnu
            templates = [
                "Le terme {entity} appara√Æt dans ce document.",
                "Nous devons traiter {entity} avec attention.",
                "L'√©l√©ment {entity} est important pour cette analyse.",
                "Il faut consid√©rer {entity} dans notre √©valuation.",
                "Le cas {entity} n√©cessite un examen approfondi."
            ]
        
        # S√©lectionne un template au hasard
        template = random.choice(templates)
        
        # Remplace le placeholder par le terme r√©el
        sentence = template.replace("{entity}", term)
        
        # Ajoute parfois du contexte suppl√©mentaire pour enrichir
        if add_context and random.random() < 0.3:  # 30% de chance
            connector = random.choice(self.connectors)
            context = random.choice(self.context_additions)
            sentence = f"{connector} {sentence.lower()} {context}."
        
        return sentence
    
    def create_spacy_annotation(self, sentence: str, term: str, 
                              entity_type: str) -> Tuple[str, Dict[str, List]]:
        """
        Cr√©e une annotation au format SpaCy pour une phrase donn√©e
        
        Args:
            sentence (str): Phrase contenant l'entit√©
            term (str): Terme √† annoter
            entity_type (str): Type d'entit√© NER
            
        Returns:
            Tuple[str, Dict]: Phrase et dictionnaire d'annotations SpaCy
        """
        # Recherche toutes les occurrences du terme dans la phrase
        entities = []
        
        # Utilise une recherche insensible √† la casse
        pattern = re.compile(re.escape(term), re.IGNORECASE)
        
        for match in pattern.finditer(sentence):
            start = match.start()
            end = match.end()
            
            # Ajoute l'annotation de l'entit√©
            entities.append((start, end, entity_type))
        
        # Format d'annotation SpaCy
        annotations = {
            "entities": entities
        }
        
        return sentence, annotations
    
    def generate_training_data(self, entity_files: Dict[str, str], 
                             sentences_per_term: int = 5,
                             add_variations: bool = True) -> List[Tuple[str, Dict]]:
        """
        G√©n√®re un ensemble complet de donn√©es d'entra√Ænement
        
        Args:
            entity_files (Dict[str, str]): Dictionnaire {type_entit√©: chemin_fichier}
            sentences_per_term (int): Nombre de phrases √† g√©n√©rer par terme
            add_variations (bool): Ajouter des variations contextuelles
            
        Returns:
            List[Tuple[str, Dict]]: Donn√©es d'entra√Ænement au format SpaCy
        """
        training_data = []
        generation_stats = {}
        
        print("üöÄ D√©but de la g√©n√©ration des donn√©es d'entra√Ænement...")
        
        for entity_type, filepath in entity_files.items():
            print(f"\nüìÇ Traitement des entit√©s de type '{entity_type}'...")
            
            try:
                # Charge les termes depuis le fichier
                terms = self.load_terms_from_file(filepath)
                generated_count = 0
                
                for term in terms:
                    # G√©n√®re plusieurs phrases pour chaque terme
                    for i in range(sentences_per_term):
                        try:
                            # G√©n√®re une phrase
                            sentence = self.generate_sentence_for_term(
                                term, entity_type, add_variations
                            )
                            
                            # Cr√©e l'annotation SpaCy
                            annotated_sentence, annotations = self.create_spacy_annotation(
                                sentence, term, entity_type
                            )
                            
                            # Ajoute aux donn√©es d'entra√Ænement
                            training_data.append((annotated_sentence, annotations))
                            generated_count += 1
                            
                        except Exception as e:
                            print(f"‚ö†Ô∏è Erreur lors de la g√©n√©ration pour '{term}': {e}")
                            continue
                
                generation_stats[entity_type] = {
                    'terms_count': len(terms),
                    'sentences_generated': generated_count
                }
                
                print(f"‚úÖ {generated_count} phrases g√©n√©r√©es pour {len(terms)} termes de type '{entity_type}'")
                
            except Exception as e:
                print(f"‚ùå Erreur lors du traitement de '{entity_type}': {e}")
                generation_stats[entity_type] = {'error': str(e)}
                continue
        
        # M√©lange les donn√©es pour un meilleur entra√Ænement
        random.shuffle(training_data)
        
        print(f"\nüéØ G√©n√©ration termin√©e: {len(training_data)} phrases d'entra√Ænement cr√©√©es")
        
        return training_data, generation_stats
    
    def save_training_data(self, training_data: List[Tuple[str, Dict]], 
                      filename: str = None) -> str:
    # """
    # Sauvegarde les donn√©es d'entra√Ænement au format JSON
    
    # Args:
    #     training_data: Donn√©es d'entra√Ænement g√©n√©r√©es
    #     filename: Nom du fichier (g√©n√©r√© automatiquement si None)
        
    # Returns:
    #     str: Chemin du fichier sauvegard√©
    # """
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"training_data_{timestamp}.json"
    
    # Cr√©e le dossier data s'il n'existe pas
        data_dir = Path("data")
        data_dir.mkdir(exist_ok=True)
    
        filepath = data_dir / filename
    
        try:
        # Convertit les donn√©es au format JSON standard
            json_data = []
            for text, annotations in training_data:
                # V√©rifie le format des donn√©es
                if not isinstance(text, str):
                    print(f"‚ö†Ô∏è Texte invalide ignor√©: {type(text)}")
                    continue
                
                if not isinstance(annotations, dict) or 'entities' not in annotations:
                    print(f"‚ö†Ô∏è Annotations invalides ignor√©es: {annotations}")
                    continue
            
                json_data.append({
                    "text": text,
                    "entities": annotations["entities"]
                })
        
            if not json_data:
                raise ValueError("Aucune donn√©e valide √† sauvegarder")
        
            # Sauvegarde au format JSON standard
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, indent=2, ensure_ascii=False)
        
            print(f"üíæ {len(json_data)} exemples sauvegard√©s dans: {filepath}")
            return str(filepath)
        
        except Exception as e:
            raise Exception(f"Erreur lors de la sauvegarde: {e}")
    
    def preview_training_data(self, training_data: List[Tuple[str, Dict]], 
                            max_examples: int = 10) -> str:
        """
        G√©n√®re un aper√ßu des donn√©es d'entra√Ænement pour pr√©visualisation
        
        Args:
            training_data: Donn√©es d'entra√Ænement
            max_examples: Nombre maximum d'exemples √† afficher
            
        Returns:
            str: Texte de pr√©visualisation format√©
        """
        if not training_data:
            return "Aucune donn√©e d'entra√Ænement disponible."
        
        preview_text = f"üìä APER√áU DES DONN√âES D'ENTRA√éNEMENT\n"
        preview_text += f"{'='*50}\n\n"
        preview_text += f"Total d'exemples g√©n√©r√©s: {len(training_data)}\n\n"
        
        # Affiche quelques exemples
        preview_text += f"üîç EXEMPLES ({min(max_examples, len(training_data))} premiers):\n"
        preview_text += f"{'-'*40}\n\n"
        
        for i, (sentence, annotations) in enumerate(training_data[:max_examples]):
            preview_text += f"Exemple {i+1}:\n"
            preview_text += f"Phrase: {sentence}\n"
            
            if annotations["entities"]:
                preview_text += "Entit√©s d√©tect√©es:\n"
                for start, end, label in annotations["entities"]:
                    entity_text = sentence[start:end]
                    preview_text += f"  - '{entity_text}' ({label}) [position {start}-{end}]\n"
            else:
                preview_text += "Aucune entit√© d√©tect√©e\n"
            
            preview_text += "\n"
        
        # Statistiques par type d'entit√©
        entity_stats = {}
        for sentence, annotations in training_data:
            for start, end, label in annotations["entities"]:
                entity_stats[label] = entity_stats.get(label, 0) + 1
        
        if entity_stats:
            preview_text += f"üìà STATISTIQUES PAR TYPE D'ENTIT√â:\n"
            preview_text += f"{'-'*30}\n"
            for entity_type, count in sorted(entity_stats.items()):
                preview_text += f"{entity_type}: {count} occurrences\n"
        
        return preview_text
    
    def validate_training_data(self, training_data: List[Tuple[str, Dict]]) -> Dict[str, Any]:
        """
        Valide la qualit√© des donn√©es d'entra√Ænement g√©n√©r√©es
        
        Args:
            training_data: Donn√©es √† valider
            
        Returns:
            Dict: Rapport de validation avec statistiques et erreurs
        """
        validation_report = {
            'total_examples': len(training_data),
            'valid_examples': 0,
            'invalid_examples': 0,
            'errors': [],
            'entity_distribution': {},
            'sentence_length_stats': {
                'min': float('inf'),
                'max': 0,
                'avg': 0
            }
        }
        
        sentence_lengths = []
        
        for i, (sentence, annotations) in enumerate(training_data):
            is_valid = True
            sentence_length = len(sentence.split())
            sentence_lengths.append(sentence_length)
            
            # V√©rifie que les annotations sont coh√©rentes
            for start, end, label in annotations.get("entities", []):
                # V√©rifie que les positions sont valides
                if start >= end or end > len(sentence):
                    validation_report['errors'].append(
                        f"Exemple {i}: Position d'entit√© invalide ({start}-{end})"
                    )
                    is_valid = False
                
                # Compte les entit√©s par type
                validation_report['entity_distribution'][label] = \
                    validation_report['entity_distribution'].get(label, 0) + 1
            
            if is_valid:
                validation_report['valid_examples'] += 1
            else:
                validation_report['invalid_examples'] += 1
        
        # Calcule les statistiques de longueur
        if sentence_lengths:
            validation_report['sentence_length_stats']['min'] = min(sentence_lengths)
            validation_report['sentence_length_stats']['max'] = max(sentence_lengths)
            validation_report['sentence_length_stats']['avg'] = sum(sentence_lengths) / len(sentence_lengths)
        
        return validation_report